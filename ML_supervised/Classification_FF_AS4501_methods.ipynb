{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#The-SDSS-QSOs-and-stars-datasets\" data-toc-modified-id=\"The-SDSS-QSOs-and-stars-datasets-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>The SDSS QSOs and stars datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-test-split\" data-toc-modified-id=\"Train-test-split-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Train test split</a></span></li></ul></li><li><span><a href=\"#Define-some-functions\" data-toc-modified-id=\"Define-some-functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Define some functions</a></span></li><li><span><a href=\"#Joint-probability-modelling-classification\" data-toc-modified-id=\"Joint-probability-modelling-classification-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Joint probability modelling classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bayes-rule\" data-toc-modified-id=\"Bayes-rule-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Bayes rule</a></span></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Naive Bayes</a></span></li><li><span><a href=\"#Gaussian-Naive-Bayes-Classifier\" data-toc-modified-id=\"Gaussian-Naive-Bayes-Classifier-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Gaussian Naive Bayes Classifier</a></span></li><li><span><a href=\"#Gaussian-Bayes-Classifier\" data-toc-modified-id=\"Gaussian-Bayes-Classifier-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Gaussian Bayes Classifier</a></span></li><li><span><a href=\"#Linear-Discriminant-Analysis-(LDA)\" data-toc-modified-id=\"Linear-Discriminant-Analysis-(LDA)-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Linear Discriminant Analysis (LDA)</a></span></li><li><span><a href=\"#Quadratic-Discriminant-Analysis-(QDA)\" data-toc-modified-id=\"Quadratic-Discriminant-Analysis-(QDA)-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Quadratic Discriminant Analysis (QDA)</a></span></li><li><span><a href=\"#Gaussian-Mixture-Model\" data-toc-modified-id=\"Gaussian-Mixture-Model-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Gaussian Mixture Model</a></span></li><li><span><a href=\"#K-nearest-neighbor-classifier\" data-toc-modified-id=\"K-nearest-neighbor-classifier-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>K-nearest neighbor classifier</a></span></li></ul></li><li><span><a href=\"#Discriminative-classification\" data-toc-modified-id=\"Discriminative-classification-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Discriminative classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Logistic regression</a></span></li><li><span><a href=\"#Support-vector-machine\" data-toc-modified-id=\"Support-vector-machine-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Support vector machine</a></span><ul class=\"toc-item\"><li><span><a href=\"#Strict-linear-separation\" data-toc-modified-id=\"Strict-linear-separation-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Strict linear separation</a></span></li><li><span><a href=\"#Soft-margin-separation\" data-toc-modified-id=\"Soft-margin-separation-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Soft margin separation</a></span></li><li><span><a href=\"#Non-linear-extension\" data-toc-modified-id=\"Non-linear-extension-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Non linear extension</a></span></li></ul></li></ul></li><li><span><a href=\"#Decision-trees\" data-toc-modified-id=\"Decision-trees-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Decision trees</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tree-splitting\" data-toc-modified-id=\"Tree-splitting-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Tree splitting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Other-classification-criteria\" data-toc-modified-id=\"Other-classification-criteria-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Other classification criteria</a></span></li><li><span><a href=\"#Stopping-criteria\" data-toc-modified-id=\"Stopping-criteria-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Stopping criteria</a></span></li></ul></li><li><span><a href=\"#Building-the-tree\" data-toc-modified-id=\"Building-the-tree-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Building the tree</a></span></li><li><span><a href=\"#Bagging,-Random-forests-and-boosting\" data-toc-modified-id=\"Bagging,-Random-forests-and-boosting-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Bagging, Random forests and boosting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bagging\" data-toc-modified-id=\"Bagging-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Bagging</a></span></li><li><span><a href=\"#Random-forests\" data-toc-modified-id=\"Random-forests-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Random forests</a></span><ul class=\"toc-item\"><li><span><a href=\"#Out-of-bag-metrics\" data-toc-modified-id=\"Out-of-bag-metrics-5.3.2.1\"><span class=\"toc-item-num\">5.3.2.1&nbsp;&nbsp;</span>Out-of-bag metrics</a></span></li><li><span><a href=\"#Feature-importance\" data-toc-modified-id=\"Feature-importance-5.3.2.2\"><span class=\"toc-item-num\">5.3.2.2&nbsp;&nbsp;</span>Feature importance</a></span></li></ul></li><li><span><a href=\"#Boosting\" data-toc-modified-id=\"Boosting-5.3.3\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Boosting</a></span></li><li><span><a href=\"#Gradient-Boosting\" data-toc-modified-id=\"Gradient-Boosting-5.3.4\"><span class=\"toc-item-num\">5.3.4&nbsp;&nbsp;</span>Gradient Boosting</a></span></li><li><span><a href=\"#Grid-search\" data-toc-modified-id=\"Grid-search-5.3.5\"><span class=\"toc-item-num\">5.3.5&nbsp;&nbsp;</span>Grid search</a></span></li></ul></li><li><span><a href=\"#Extreme-Gradient-Boosting-(xgboost)\" data-toc-modified-id=\"Extreme-Gradient-Boosting-(xgboost)-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Extreme Gradient Boosting (xgboost)</a></span></li><li><span><a href=\"#Light-Gradient-Boosting-Microsoft-(lightGBM)\" data-toc-modified-id=\"Light-Gradient-Boosting-Microsoft-(lightGBM)-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Light Gradient Boosting Microsoft (lightGBM)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transform-labels-to-numbers-with-LabelEncoder\" data-toc-modified-id=\"Transform-labels-to-numbers-with-LabelEncoder-5.5.1\"><span class=\"toc-item-num\">5.5.1&nbsp;&nbsp;</span>Transform labels to numbers with LabelEncoder</a></span></li></ul></li><li><span><a href=\"#XGBoost\" data-toc-modified-id=\"XGBoost-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>XGBoost</a></span></li><li><span><a href=\"#LightGBM\" data-toc-modified-id=\"LightGBM-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>LightGBM</a></span></li></ul></li><li><span><a href=\"#Comparison-of-methods\" data-toc-modified-id=\"Comparison-of-methods-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Comparison of methods</a></span></li><li><span><a href=\"#Imbalanced-data-sets\" data-toc-modified-id=\"Imbalanced-data-sets-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Imbalanced data sets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Reload-data-set\" data-toc-modified-id=\"Reload-data-set-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Reload data set</a></span></li><li><span><a href=\"#SMOTE:-Synthetic-Minority-Over-sampling-Technique\" data-toc-modified-id=\"SMOTE:-Synthetic-Minority-Over-sampling-Technique-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>SMOTE: Synthetic Minority Over-sampling Technique</a></span></li><li><span><a href=\"#ADASYN\" data-toc-modified-id=\"ADASYN-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>ADASYN</a></span></li><li><span><a href=\"#LightGBM-using-the-resampled-dataset\" data-toc-modified-id=\"LightGBM-using-the-resampled-dataset-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>LightGBM using the resampled dataset</a></span></li><li><span><a href=\"#Confusion-matrix-without-resampling\" data-toc-modified-id=\"Confusion-matrix-without-resampling-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Confusion matrix without resampling</a></span></li><li><span><a href=\"#Confusion-matrix-with-resampling\" data-toc-modified-id=\"Confusion-matrix-with-resampling-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Confusion matrix with resampling</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Introduction to supervised machine learning II</font>\n",
    "\n",
    "Francisco Förster Burón, CMM-U.Chile / MAS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary intro to supervised classification**\n",
    "\n",
    "* Different diagnostics to measure the quality of a classifier beyond its accuracy.\n",
    "\n",
    "* Recall and precision and variations the key diagnostics, F1 and F$_\\beta$ harmonic mean averages (incl. weights).\n",
    "\n",
    "* Macro and micro averages can be used for all diagnostics\n",
    "\n",
    "* ROC and DET curves allows visualizing trade--off between false positives and false negatives\n",
    "\n",
    "* Area under the curve measures overall quality.\n",
    "\n",
    "* Training, validation and test sets important to allow model selection without overfitting (knowledge leaking)\n",
    "\n",
    "* Different techniques to avoid very small validation and test sets: variations on K-fold cross-validation (stratified important for unbalanced sets)\n",
    "\n",
    "* Training with unbalanced sets may require weighting the sample\n",
    "\n",
    "* Remember that you can train with the full dataset after all the previous tests in order to build a classifier which will be used in some other data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bibliography**: \n",
    "\n",
    "Statistics, Data Mining, and Machine Learning in Astronomy: A Practical Python Guide for the Analysis of Survey Data (Princeton Series in Modern Observational Astronomy), Željko Ivezic, Andrew J. Connolly, Jacob T VanderPlas, Alexander Gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:40:55.170137Z",
     "start_time": "2023-04-21T14:40:55.163358Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:40:56.706773Z",
     "start_time": "2023-04-21T14:40:56.030211Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:40:56.899866Z",
     "start_time": "2023-04-21T14:40:56.712193Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:00.581854Z",
     "start_time": "2023-04-21T14:40:56.905608Z"
    }
   },
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "from holoviews.operation.datashader import datashade, dynspread\n",
    "import datashader as ds\n",
    "from matplotlib import cm\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The SDSS QSOs and stars datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download files SDSS_stars.csv and SDSS_QSO.dat from https://astrostatistics.psu.edu/MSMA/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:00.951475Z",
     "start_time": "2023-04-21T14:41:00.934711Z"
    }
   },
   "outputs": [],
   "source": [
    "stars = pd.read_csv(\"data/SDSS_stars.csv\")\n",
    "stars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:02.391108Z",
     "start_time": "2023-04-21T14:41:02.387893Z"
    }
   },
   "outputs": [],
   "source": [
    "stars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:02.936593Z",
     "start_time": "2023-04-21T14:41:02.800588Z"
    }
   },
   "outputs": [],
   "source": [
    "QSOs = pd.read_csv(\"data/SDSS_QSO.dat\", sep = \"\\s+\", index_col = \"SDSS\")\n",
    "QSOs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:04.096042Z",
     "start_time": "2023-04-21T14:41:04.093218Z"
    }
   },
   "outputs": [],
   "source": [
    "QSOs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:04.936364Z",
     "start_time": "2023-04-21T14:41:04.927465Z"
    }
   },
   "outputs": [],
   "source": [
    "stars[\"cat\"] = \"star\"\n",
    "stars[\"u-g\"] = stars.u_mag - stars.g_mag\n",
    "stars[\"g-r\"] = stars.g_mag - stars.r_mag\n",
    "stars[\"r-i\"] = stars.r_mag - stars.i_mag\n",
    "stars[\"i-z\"] = stars.i_mag - stars.z_mag\n",
    "QSOs[\"cat\"] = \"QSO\"\n",
    "QSOs[\"u-g\"] = QSOs.u_mag - QSOs.g_mag\n",
    "QSOs[\"g-r\"] = QSOs.g_mag - QSOs.r_mag\n",
    "QSOs[\"r-i\"] = QSOs.r_mag - QSOs.i_mag\n",
    "QSOs[\"i-z\"] = QSOs.i_mag - QSOs.z_mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new pandas dataframe with both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:05.909067Z",
     "start_time": "2023-04-21T14:41:05.889149Z"
    }
   },
   "outputs": [],
   "source": [
    "sel_cols = [\"u-g\", \"g-r\", \"cat\"]\n",
    "data = pd.concat([stars[sel_cols], QSOs[sel_cols].sample(5000, random_state=1)])\n",
    "#data = pd.concat([stars[sel_cols], QSOs[sel_cols]])\n",
    "data[\"cat\"] = data[\"cat\"].astype(\"category\")\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:07.234376Z",
     "start_time": "2023-04-21T14:41:07.226691Z"
    }
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:07.764123Z",
     "start_time": "2023-04-21T14:41:07.758438Z"
    }
   },
   "outputs": [],
   "source": [
    "data.groupby(\"cat\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:08.787196Z",
     "start_time": "2023-04-21T14:41:08.384961Z"
    }
   },
   "outputs": [],
   "source": [
    "points = hv.Points(data, kdims=['u-g', 'g-r'])\n",
    "datashade.x_range=(data[\"u-g\"].quantile(0.001),data[\"u-g\"].quantile(0.995))\n",
    "datashade.y_range=(data[\"g-r\"].quantile(0.001),data[\"g-r\"].quantile(0.995)); \n",
    "datashade.cmap=cm.viridis\n",
    "dynspread.max_px=100\n",
    "dynspread.threshold=0.5\n",
    "dynspread(datashade(points)).opts(width=800, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this in the stars vs QSOs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:03:06.766219Z",
     "start_time": "2023-04-21T15:03:06.762046Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[[\"u-g\", \"g-r\"]], data.cat, test_size=.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the labels numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:03:07.543013Z",
     "start_time": "2023-04-21T15:03:07.537351Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:03:09.061925Z",
     "start_time": "2023-04-21T15:03:09.057034Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data.cat)\n",
    "le.classes_\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:03:09.861160Z",
     "start_time": "2023-04-21T15:03:09.857180Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:03:10.566984Z",
     "start_time": "2023-04-21T15:03:10.563158Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:15.272200Z",
     "start_time": "2023-04-21T14:41:15.266533Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.3f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:17.319214Z",
     "start_time": "2023-04-21T14:41:17.316687Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:18.562565Z",
     "start_time": "2023-04-21T14:41:18.559073Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_contours(ax, clf, xx, yy, pos_label, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) == pos_label\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint probability modelling classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bayes rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of data {${\\bf x}$} of N points in D dimensions, such that $x_i^j$ is the $j$th feature of the $i$th point, and a set of discrete labels {$y$} drawn from $K$ classes, we can write the joint probability of point $i$th being in class $k$ and having features ${\\bf x_i}$ as:\n",
    "\n",
    "$\\Large p(y_k, {\\bf x_i}) =  p(y_k | {\\bf x_i}) p({\\bf x}_i) = p({\\bf x}_i | y_k) p(y_k)$\n",
    "\n",
    "which implies Bayes theorem:\n",
    "\n",
    "$\\Large p(y_k | {\\bf x_i}) = \\frac{p({\\bf x}_i | y_k) p(y_k)}{p({\\bf x}_i)} =  \\frac{p({\\bf x}_i | y_k) p(y_k)}{\\sum_j p({\\bf x}_i | y_j) p(y_j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If we make the strong assumption that all attributes are **conditionally** independent we can write:\n",
    "\n",
    "$\\Large p(x^i, x^j | y_k) = p(x^i | y_k) p (x^j | y_k)$,\n",
    "\n",
    "This can be expressed as:\n",
    "\n",
    "$\\Large p(x^0, x^1, x^2, ... , x^N | y_k) = \\prod_j p(x^j | y_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Applying Bayes' rule this can be written as:\n",
    "\n",
    "$\\Large p(y_k | x^0, x^1, x^2, ... , x^N) = \\frac{p(x^0, x^1, x^2, ... , x^N | y_k) p(y_k)}{\\sum_j p(x^0, x^1, x^2, ... , x^N|y_j) p(y_j)}$\n",
    "\n",
    "which assuming conditional independence becomes:\n",
    "\n",
    "$\\Large p(y_k | x^0, x^1, x^2, ... , x^N) = \\frac{\\prod_i p(x^i | y_k) p(y_k)}{\\sum_j \\prod_i p(x^i | y_j) p(y_j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous expression we can compute the most likely value of $y$ by maximizinng over $y_k$:\n",
    "\n",
    "$\\Large \\hat y = argmax_{y_k} \\frac{\\prod_i p(x^i | y_k) p(y_k)}{\\sum_j \\prod_i p(x^i | y_j) p(y_j)} = argmax_{y_k} \\frac{\\prod_i p_k(x^i) \\pi_k}{\\sum_j \\prod_i p_j(x^i) \\pi_j}$\n",
    "\n",
    "defining \n",
    "\n",
    "$\\Large p_k(x^i) \\equiv p(x^i | y_k)$ and $\\Large \\pi_k \\equiv p(y_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is then to **determine $p_k(x^i)$ and $\\pi_k$ from a training set.**\n",
    "\n",
    "This is usually done with parametrized models, or more general parametric or non parametric density estimation techniques.\n",
    "\n",
    "Very simple example: when the features are categorical. For each label $y_k$ in the training set, the maximum likelihood estimate of the probability for feature $x^i$ is the number of objects with a particular value $x^i$ divided by the total number of objects with the same label. The values of $\\pi_k$ are the fraction of data with $y = y_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the probabilities $p_k(x^i)$ are modeled as one dimensional normal distribution:\n",
    "\n",
    "$\\Large p_k(x_i) = \\frac{1}{\\sqrt{2 \\pi (\\sigma_k^i)^2}} \\exp \\biggl( -\\frac{(x_i - \\mu^i_k)^2}{2 (\\sigma_k^i)^2}\\biggr)$\n",
    "\n",
    "$\\Rightarrow$\n",
    "\n",
    "Using this model, the maximum likelihood estimator becomes:\n",
    "\n",
    "$\\Large \\hat{y} = argmax_{y_k} \\biggl[ \\ln \\pi_k - \\frac{1}{2} \\sum_1^N \\ln \\bigg(2 \\pi (\\sigma_k^i)^2\\biggr) + \\frac{(x^i - \\mu_k^i)^2}{(\\sigma_k^i)^2} \\biggr]$\n",
    "\n",
    "*Note error in 9.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Example of Gaussian Naive Bayes classification:\n",
    " ![](images/NaiveBayes.png)\n",
    " ![](images/NaiveBayes_class.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:28.930550Z",
     "start_time": "2023-04-21T14:41:28.909877Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"f1-score:\", metrics.f1_score(le.inverse_transform(y_test), le.inverse_transform(y_pred), pos_label=\"QSO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:30.047248Z",
     "start_time": "2023-04-21T14:41:29.887230Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics.confusion_matrix(le.inverse_transform(y_test), le.inverse_transform(y_pred)), [\"star\", \"QSO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:32.877471Z",
     "start_time": "2023-04-21T14:41:32.586730Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "xx, yy = make_meshgrid(data[\"u-g\"], data[\"g-r\"])\n",
    "plot_contours(ax, gnb, xx, yy, le.transform([\"QSO\"]),\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.5)\n",
    "ax.scatter(data[\"u-g\"], data[\"g-r\"], c=(data.cat=='QSO'), cmap=plt.cm.coolwarm, s=10, edgecolors=None, alpha = 0.3)\n",
    "ax.set_xlabel(\"u-g\", fontsize=16)\n",
    "ax.set_ylabel(\"g-r\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gaussian Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to relax the assumption of conditional independence. This can be done including covariances in the model distributions.\n",
    "\n",
    "A multivariate Gaussian can be expressed as:\n",
    "\n",
    "$\\Large p_k({\\bf x}) = \\frac{1}{\\bigl|\\sum_k\\bigr|^{1/2} (2 \\pi)^{D/2}} \\exp \\bigg\\lbrace -\\frac{1}{2} ({\\bf x} - \\mu_k)^T \\sum_k^{-1} ({\\bf x} - \\mu_k) \\biggr\\rbrace$\n",
    "\n",
    "where $\\sum_k$ is a $D x D$ symmetric covariance matrix with determinant $\\bigl|\\sum_k\\bigr|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this result, the Gaussian Bayes classifier estimator for $\\hat{y}$ is:\n",
    "\n",
    "$\\Large \\hat{y} = argmax_k \\biggl\\lbrace -\\frac{1}{2} \\log \\bigl|\\sum_k\\bigr| - \\frac{1}{2} ({\\bf x} - \\mu_k)^T \\sum_k^{-1} ({\\bf x} - \\mu_k) + \\log \\pi_k \\biggr\\rbrace$\n",
    "\n",
    "or (in the case of two classes)\n",
    "\n",
    "$\\Large \\hat{y} =  \\begin{cases} 1, & \\text{if}\\ m_1^2 < m_0^2 + 2 \\log \\biggl(\\frac{\\pi_1}{\\pi_0}\\biggr) + \\biggl(\\frac{|\\sum_1|}{|\\sum_0|}\\biggr) \\\\\n",
    "      0, & \\text{otherwise} \\end{cases}$\n",
    "    \n",
    "\n",
    "where $\\Large m_k^2 = (x - \\mu_k)^T \\sum_k^{-1} (x - \\mu_k)$ is the Mahalanobis distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we assume that the distributions of $p_k(\\vec{x})$ have the same covariance matrices.\n",
    "\n",
    "The optimal classifier can be derived from the log of the class posteriors:\n",
    "\n",
    "$\\Large g_k({\\bf x}) = {\\bf x}^T \\sum^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\sum^{-1} \\mu_k + \\log \\pi_k + const$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T03:37:13.851856Z",
     "start_time": "2018-10-26T03:37:13.848519Z"
    }
   },
   "source": [
    "Note that we have used that $\\large (x^T \\sum^{-1} \\mu_k) = (x^T \\sum^{-1} \\mu_k)^T = \\mu_k^T (\\sum^{-1})^T x = \\mu_k^T (\\sum^T)^{-1} x = \\mu_k^T \\sum^{-1} x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminant boundary between the classes is the line that minimizes the overlap between the Gaussians, i.e. it is linear in $\\bf x$:\n",
    "\n",
    "$\\Large g_k({\\bf x}) - g_l({\\bf x}) = {\\bf x}^T \\sum^{-1} (\\mu_k - \\mu_l) - \\frac{1}{2} \\biggl(\\mu_k^T \\sum^{-1} \\mu_k - \\mu_l^T \\sum^{-1} \\mu_l\\biggr) + \\log \\biggl( \\frac{\\pi_k}{\\pi_l} \\biggr) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:38.174849Z",
     "start_time": "2023-04-21T14:41:38.101701Z"
    }
   },
   "outputs": [],
   "source": [
    "# example\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "y_pred = lda.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"f1-score:\", metrics.f1_score(le.inverse_transform(y_test), le.inverse_transform(y_pred), pos_label=\"QSO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:38.826107Z",
     "start_time": "2023-04-21T14:41:38.730325Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics.confusion_matrix(le.inverse_transform(y_test), le.inverse_transform(y_pred)), [\"star\", \"QSO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:39.607530Z",
     "start_time": "2023-04-21T14:41:39.367492Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "xx, yy = make_meshgrid(data[\"u-g\"], data[\"g-r\"])\n",
    "plot_contours(ax, lda, xx, yy, le.transform([\"QSO\"]),\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.5)\n",
    "ax.scatter(data[\"u-g\"], data[\"g-r\"], c=(data.cat=='QSO'), cmap=plt.cm.coolwarm, s=20, edgecolors=None, alpha = 0.3)\n",
    "ax.set_xlabel(\"u-g\", fontsize=16)\n",
    "ax.set_ylabel(\"g-r\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis (QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't assume that the covariances are the same between classes, the discriminant function between classes becomes quadratic in $\\bf x$:\n",
    "\n",
    "$\\Large g({\\bf x}) = -\\frac{1}{2} \\log |\\sum_k| - \\frac{1}{2} ({\\bf x} - \\mu_k)^T \\sum_k^{-1} ({\\bf x} - \\mu_k) + \\log \\pi_k$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/LDA_QDA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:43.279875Z",
     "start_time": "2023-04-21T14:41:43.259777Z"
    }
   },
   "outputs": [],
   "source": [
    "# example\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "y_pred = qda.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"f1-score:\", metrics.f1_score(le.inverse_transform(y_test), le.inverse_transform(y_pred), pos_label=\"QSO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:43.937417Z",
     "start_time": "2023-04-21T14:41:43.838759Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics.confusion_matrix(y_test, y_pred), [\"star\", \"QSO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:41:45.224066Z",
     "start_time": "2023-04-21T14:41:45.018255Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "xx, yy = make_meshgrid(data[\"u-g\"], data[\"g-r\"])\n",
    "plot_contours(ax, qda, xx, yy, le.transform([\"QSO\"]),\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.5)\n",
    "ax.scatter(data[\"u-g\"], data[\"g-r\"], c=(data.cat=='QSO'), cmap=plt.cm.coolwarm, s=20, edgecolors=None, alpha = 0.3)\n",
    "ax.set_xlabel(\"u-g\", fontsize=16)\n",
    "ax.set_ylabel(\"g-r\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See http://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html#sphx-glr-auto-examples-classification-plot-lda-qda-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further assume that the distribution of features in each classes can be modeled by a mixture of Gaussians, or even more generally by a multidimensional kernel density estimation for a non-parametric approach.\n",
    "\n",
    "The first case is called **Gaussian Mixture Model classification** and the second case **Kernel discriminant analysis**.\n",
    "\n",
    "The kernel discriminant analysis can be thought as the limit of the Gaussian Mixture Model where every point is represented by a Gaussian. In this case the optimization of the model is simply over the kernel bandwidth, so it is computationally more simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/GMM_class.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T03:45:11.637450Z",
     "start_time": "2018-10-26T03:45:11.631684Z"
    },
    "collapsed": true
   },
   "source": [
    "See http://scikit-learn.org/0.16/auto_examples/mixture/plot_gmm_classifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbor classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most widely used and powerful classifiers: simply uses the label of the nearest point!\n",
    "\n",
    "Intuitive justification is that  $\\Large p(y|x) \\approx p(y|x')$  if  $\\Large x \\approx x'$.\n",
    "\n",
    "Method is completely **non-parametric**\n",
    "\n",
    "Resulting decision boundary is a Voronoi tesselation of the attribute space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of neighbors K is used as a smoothing parameter to regulate the complexity of the classification.\n",
    "\n",
    "Weights can be assigned by weighting the votes by their distance.\n",
    "\n",
    "![](images/Knearest.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**\n",
    "\n",
    "A metric must be defined to compute a multidimensional distance.\n",
    "\n",
    "When features have different dimensions this can be arbitrary. \n",
    "\n",
    "Some approaches:\n",
    "\n",
    "- normalization of the features (scaling from 0 to 1)\n",
    "- weighting the importance of features based on cross-validation\n",
    "- use of Mahalanobis distance: $\\Large D(x, x_0) = (x-x_0)^T C^{-1} (x-x_0)$\n",
    "\n",
    "This method works well when the training sample is large, when this is not the case a parametric classification model can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the optimal number of neighbors via stratified k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:53:48.192878Z",
     "start_time": "2023-04-21T14:53:48.188712Z"
    }
   },
   "outputs": [],
   "source": [
    "le.transform(['QSO'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:53:51.840827Z",
     "start_time": "2023-04-21T14:53:49.361834Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "n_splits = 4\n",
    "acc = {}\n",
    "f1s = {}\n",
    "kf = StratifiedKFold(n_splits = n_splits)\n",
    "nns = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "for nn in nns:\n",
    "    acc[nn] = []\n",
    "    f1s[nn] = []\n",
    "    for idx, traintest in enumerate(kf.split(data[[\"u-g\", \"g-r\"]], data.cat)):\n",
    "        train, test = traintest\n",
    "        knn = KNeighborsClassifier(n_neighbors=nn)\n",
    "        y_pred = knn.fit(X_train, y_train).predict(X_test)\n",
    "        acc[nn].append(metrics.accuracy_score(y_test, y_pred))\n",
    "        f1s[nn].append(metrics.f1_score(le.inverse_transform(y_test), le.inverse_transform(y_pred), pos_label=\"QSO\"))\n",
    "    acc[nn] = np.average(acc[nn])\n",
    "    f1s[nn] = np.average(f1s[nn])\n",
    "    print(\"Accuracy (%i):\" % nn, metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"f1-score (%i):\" % nn, metrics.f1_score(le.inverse_transform(y_test), le.inverse_transform(y_pred), pos_label=\"QSO\"))\n",
    "acc = list(acc.values())\n",
    "f1s = list(f1s.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:53:52.932016Z",
     "start_time": "2023-04-21T14:53:52.853221Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(nns, acc, c = 'r')\n",
    "plt.scatter(nns, f1s, c = 'b')\n",
    "plt.ylabel(\"accuracy|f1-score\")\n",
    "plt.xlabel(\"n neighbors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:53:56.374879Z",
     "start_time": "2023-04-21T14:53:56.289241Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics.confusion_matrix(y_test, y_pred), [\"star\", \"QSO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:54:02.042350Z",
     "start_time": "2023-04-21T14:53:57.375146Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "xx, yy = make_meshgrid(data[\"u-g\"], data[\"g-r\"])\n",
    "plot_contours(ax, knn, xx, yy, le.transform([\"QSO\"]),\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.5)\n",
    "ax.scatter(data[\"u-g\"], data[\"g-r\"], c=(data.cat=='QSO'), cmap=plt.cm.coolwarm, s=20, edgecolors=None, alpha = 0.3)\n",
    "ax.set_xlabel(\"u-g\", fontsize=16)\n",
    "ax.set_ylabel(\"g-r\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct modelling of the decision boundary: $\\large g(x) = p(y = 1|x)$. The boundary would be defined by\n",
    "\n",
    "\\begin{equation}\n",
    "\\Large\n",
    "\\hat y = \\begin{cases} 1  \\ \\ \\ {\\rm if}\\  g(x) > 0.5 \\\\ 0 \\ \\ \\ {\\rm otherwise} \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of discriminative analysis where the discriminant function is modelled as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large p(y = 1|x) = \\frac{\\exp \\biggl[ \\sum_j \\theta_j x^j \\biggr]}{1 + \\exp \\biggl[ \\sum_j \\theta_j x^j \\biggr]} = p({\\bf \\theta})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:54:05.738003Z",
     "start_time": "2023-04-21T14:54:05.485283Z"
    }
   },
   "outputs": [],
   "source": [
    "xt = np.linspace(-10, 10)\n",
    "plt.plot(xt, np.exp(xt) / (1 + np.exp(xt)))\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(r\"$p(y=1|x)$\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that this assumptions makes the log likelihood ratio linear in the parameters:\n",
    "\n",
    "$\\Large \\log \\biggl( \\frac{p(y = 1 | x)}{p(y = 0 | x)} \\biggr) = \\beta_0 + \\beta_1 x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/logistic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See http://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html#sphx-glr-auto-examples-linear-model-plot-iris-logistic-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:54:07.616881Z",
     "start_time": "2023-04-21T14:54:07.564478Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lrc = LogisticRegression()\n",
    "y_pred = lrc.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"f1-score:\", metrics.f1_score(le.inverse_transform(y_test), le.inverse_transform(y_pred), pos_label=\"QSO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:54:08.187789Z",
     "start_time": "2023-04-21T14:54:08.080426Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics.confusion_matrix(y_test, y_pred), [\"star\", \"QSO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:54:38.698494Z",
     "start_time": "2023-04-21T14:54:38.468023Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "xx, yy = make_meshgrid(data[\"u-g\"], data[\"g-r\"])\n",
    "plot_contours(ax, lrc, xx, yy, 1,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.5)\n",
    "ax.scatter(data[\"u-g\"], data[\"g-r\"], c=(data.cat=='QSO'), cmap=plt.cm.coolwarm, s=20, edgecolors=None, alpha = 0.3)\n",
    "ax.set_xlabel(\"u-g\", fontsize=16)\n",
    "ax.set_ylabel(\"g-r\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the hyperplane that maximizes the margin $m$ between points:\n",
    "\n",
    "![](images/SVM_Ivezic.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strict linear separation\n",
    "\n",
    "* An hyperplane can be defined as ${\\bf w} \\cdot {\\bf x} - b = 0$, the distance from the origin to the hyperplane is $\\frac{b}{|| w ||}$\n",
    "\n",
    "* The equation ${\\bf w} \\cdot {\\bf x} - b = 1$ defines an hyperplane separated by $\\frac{1}{||w||}$ from the previous plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/SVM_margin.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-26T14:25:21.458524Z",
     "start_time": "2018-10-26T14:25:21.451666Z"
    }
   },
   "source": [
    "* if we have two classes, we can assign $y_i = \\pm 1$ to the points of each class.\n",
    "\n",
    "* Then, we want to find the parameters ${\\bf w}$ and $b$ such that:\n",
    "\n",
    "    $\\Large y_i ({\\bf w} \\cdot {\\bf x_i} - b) \\ge 1, ~~~~\\forall i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to find the hyperplane defined by ${\\bf w}$ and $b$ that has the largest separation between classes, i.e. we want to solve for:\n",
    "\n",
    "$\\Large \\min_{{\\bf w}, b}~ ||{\\bf w} ||,~~~ s.t. ~~~ \\Large y_i ({\\bf w} \\cdot {\\bf x_i} - b) \\ge 1, ~~~~\\forall i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to this problem can be done via quadratic programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft margin separation\n",
    "\n",
    "* If we allow for some points to cross the boundary we can instead minimize:\n",
    "\n",
    "$\\Large \\min_{{\\bf w}, b} ~\\lbrace \\frac{1}{n} \\sum_{i=1}^n \\max (0, 1 - y_i ({\\bf w} \\cdot {\\bf x} - b)) \\rbrace + \\lambda ||{\\bf w}||^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/SVM_Ivezic2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non linear extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SVM can be extended to non linear separations applying the **kernel trick**.\n",
    "\n",
    "* This consists of replacing all the dot products by a function $k({\\bf x_i}, {\\bf x_j})$\n",
    "\n",
    "* The kernel is related to the transform $\\varphi({\\bf x})$ by the equation $k({\\bf x_i}, {\\bf x_j}) = \\varphi({\\bf x_i}) \\cdot \\varphi({\\bf x_j})$\n",
    "\n",
    "* ${\\bf w}$ can be written in the transformed space: ${\\bf w} = \\sum_i \\alpha_i y_i \\varphi({\\bf x_i})$\n",
    "\n",
    "* Dot product with $\\bf w$ can be written as ${\\bf w} \\cdot \\varphi({\\bf x}) = \\sum_i \\alpha_i y_i k({\\bf x_i}, {\\bf x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/Kernel_Machine.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common kernel is the following:\n",
    "\n",
    "$\\Large K(x_i, x_i') = \\exp(-\\gamma || x_i - x_i'||^2)$\n",
    "\n",
    "where $\\gamma$ is learnt through cross--validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/SVM_kernel.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:55:32.903619Z",
     "start_time": "2023-04-21T14:55:32.712523Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_model = SVC(gamma='auto')#, kernel = \"linear\")\n",
    "y_pred = svc_model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"f1-score:\", metrics.f1_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:55:36.274715Z",
     "start_time": "2023-04-21T14:55:36.183230Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics.confusion_matrix(y_test, y_pred), [\"star\", \"QSO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:55:51.398182Z",
     "start_time": "2023-04-21T14:55:48.029465Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "xx, yy = make_meshgrid(data[\"u-g\"], data[\"g-r\"])\n",
    "plot_contours(ax, svc_model, xx, yy, 1,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.5)\n",
    "ax.scatter(data[\"u-g\"], data[\"g-r\"], c=(data.cat=='QSO'), cmap=plt.cm.coolwarm, s=20, edgecolors=None, alpha = 0.3)\n",
    "ax.set_xlabel(\"u-g\", fontsize=16)\n",
    "ax.set_ylabel(\"g-r\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extension of classification methods based on modelling the boundary between classes are the **decision trees**, which model the decision boundary in a hierarchical fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/sorted.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the decision boundary for the following design matrix?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:55:54.893090Z",
     "start_time": "2023-04-21T14:55:54.791208Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "ax.scatter(np.random.random(100), np.random.random(100), c = 'r')\n",
    "ax.scatter(1 + np.random.random(100), 1 + np.random.random(100), c = 'r')\n",
    "ax.scatter(np.random.random(100), 1 + np.random.random(100), c = 'b')\n",
    "ax.scatter(1 + np.random.random(100), np.random.random(100), c = 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This type of decision trees can also be applied for the classification of astronomical sources**\n",
    "\n",
    "- At each branch data is subdivided into child nodes, based on a decision boundary.\n",
    "\n",
    "- Boundaries are usually axis aligned (data is split along one feature at each level of the tree)\n",
    "\n",
    "- the splitting occurs until some stopping criteria\n",
    "\n",
    "- terminal nodes record the fraction of points contained within that node from each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/tree.png)\n",
    "\n",
    "At each node the fraction of points within the node in each class is annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/tree_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficulty in building a tree is which feature to choose and the value for splitting in each node. \n",
    "\n",
    "A simple splitting criterion is based on entropy:\n",
    "\n",
    "\n",
    "$\\Large E(x) = -\\sum\\limits_i p_i(x) \\ln(p_i(x))$\n",
    "\n",
    "\n",
    "where $i$ is the class and $p_i(x)$ is the probability of that class given the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above the entropy at the first node is ![](images/split_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:55:58.169662Z",
     "start_time": "2023-04-21T14:55:58.165625Z"
    }
   },
   "outputs": [],
   "source": [
    "def E(n1, n2):\n",
    "    p1 = n1 / (n1 + n2)\n",
    "    p2 = n2 / (n1 + n2)\n",
    "    return -(p1 * np.log(p1) + p2 * np.log(p2))\n",
    "print(E(69509, 346))\n",
    "print(E(49509, 20346))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define **information gain** as the reduction of entropy due to the partitioning of the data (the difference between the entropy of the parent node and the sum of the entropies of the child nodes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a binary split with $i=0$ representing points below the threshold and $i=1$ above the split threshold, the information gain $IG$ is:\n",
    "\n",
    "$\\Large IG(x|x_i) = E(x) - \\sum\\limits_{i=0}^{1} \\frac{N_i}{N} E(x_i)$\n",
    "\n",
    "\n",
    "where $N_i$ is the number of points, $x_i$, in class  $i$, and $E(x)$ is the entropy associated with that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above the information gain after the first split is: ![](images/split.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:56:00.091810Z",
     "start_time": "2023-04-21T14:56:00.088251Z"
    }
   },
   "outputs": [],
   "source": [
    "n1 = 69509\n",
    "n2 = 346\n",
    "n1_1 = 66668\n",
    "n2_1 = 13\n",
    "n1_2 = 2841\n",
    "n2_2 = 333\n",
    "IG = E(n1, n2) \\\n",
    "    - (n1_1 + n2_1) / (n1 + n2) * E(n1_1, n2_1) \\\n",
    "    - (n1_2 + n2_2) / (n1 + n2) * E(n1_2, n2_2)\n",
    "print(IG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the optimal splits is considered computationally intractable. \n",
    "\n",
    "The search for the split is done in a *greedy* fashion, i.e. each feature is considered one at a time and the feature that provides the largest information gain is split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the feature at which to split the data is analogously defined, sorting the data on feature $i$ and maximizing the information gain for a given split point, $s$:\n",
    "\n",
    "\n",
    "$\\begin{align}\n",
    "& \\Large IG(x|s) =\n",
    "& \\Large \\arg\\max\\limits_s \\biggl(E(x) -  \\frac{N(x~|~x ~<~ s)}{N} E(x~|~x ~<~ s) - \\frac{N(x~|~x ~\\ge~ s)}{N} E(x~|~x ~\\ge~ s) \\biggr)\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other classification criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other classification criteria are the Gini coefficient and the misclassification error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Gini coefficient (G)** is the probability that a source would be incorrectly classified if it was chosen at random and the label was selected randomly based on the distribution of classifications\n",
    "within the data set, for $k$ classes is given by:\n",
    "\n",
    "$\\Large G = \\sum\\limits_i^k p_i (1 - p_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **misclassification error (MC)** is the fractional probability that a point selected at random will be missclassified:\n",
    "\n",
    "$\\Large MC = 1 - \\max_i(p_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the tree nodes are split until some stopping criteria is met. Common stopping criteria are:\n",
    "\n",
    "- a node contains only one class of objects\n",
    "- the split does not provide positive information gain or a reduction in the misclassification error\n",
    "- the number of objects per node reaches a predefined value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complexity of a tree is defined by the number of levels of depth of the tree. As the depth increases, the error on the training set will decrease, but at some point we will **overfit** the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The avoid overfitting we can use **cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach is to determine the depth (complexity) of the tree by cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/trees_crossvalidation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second approach is to **prune** the tree using cross-validation. \n",
    "\n",
    "For each node we consider whether terminating the tree at that node improves the accuracy in the validation set. \n",
    "\n",
    "If so, we prune the branches below that node\n",
    "\n",
    "![](images/tree_pruned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging, Random forests and boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different approach for classification is that of *ensemble learning*: combining the output of several models through some kind of voting or averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "In **bagging** we take averages over the prediction of a series of **bootstrap** samples from the training set. Bagging is short for bootstrap + aggregating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sample of N points in a training set, bagging generates $K$ equally sized bootstrap samples from which to estimate a function $f_i(x)$. The final estimator is then:\n",
    "\n",
    "$\\Large f(x) = \\frac{1}{K} \\sum\\limits_i^K f_i(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests\n",
    "\n",
    "In **random forests** this concept is expanded even further by generating a set of decision trees from the bootstrap samples with a random set of features.\n",
    "\n",
    "Random forests contain $n$ trees and $m$ randomly selected features used per tree. Keeping $m$ small compared to the number of features controls the complexity of the model and reduces over-fitting.\n",
    "\n",
    "Random forests and its derivatives are usually the best classifiers available for feature based classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](images/RF.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In random forests, the number of trees, the depth of the trees, and the number of features per tree, $m$, can be chosen via cross-validation.\n",
    "\n",
    "- $m$ is usually chosen to be $\\sim \\sqrt{K}$, where $K$ is the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-bag metrics\n",
    "\n",
    "In random forests we can also report **out-of-bag (oob)** metrics, which are the metrics computed over points which were not used in the bootstrap sample used to train a specific tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance\n",
    "\n",
    "One advantage of decision trees and random forests is that we can compute the change in entropy, gini index change or misclassification error summed over specific features of the tree, then average over all trees in the case of random forests.\n",
    "\n",
    "This means that we can estimate the **feature importance** over all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important options in sklearn:\n",
    "\n",
    "- `n_estimators`: the number of trees (default=10)\n",
    "\n",
    "- `criterion`: the function used to split the tree (default='gini', better use 'entropy')\n",
    "\n",
    "- `max_depth`: maximum depth of the tree\n",
    "\n",
    "- `n_jobs`: number of parallel cores to use during computation\n",
    "\n",
    "- `oob_score`: whether to use out-of-bag samples to estimates the generalization accuracy\n",
    "\n",
    "- `class_weight`: weights associated with classes in the form `{class_label: weight}` (default=None). If not given, all classes are assumed to have weight one. Can give dictionary of weights, or 'balanced' for inverse class frequency weighting, or 'balanced_subsample' for inverse class frequency weighting inside each bootstrap sample.\n",
    "\n",
    "When the tree has been trained, we can extract:\n",
    "\n",
    "- `feature_importances_`: the importance of each feature\n",
    "\n",
    "- `oob_score`: the chosen metric over out-of-bag estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some considerations**\n",
    "\n",
    "- when the number of features is large and the number of relevant features is small, the random forest may perform poorly (the chances of selecting a relevant feature in each split will be low)\n",
    "\n",
    "- increasing the number of trees generally does not lead to over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it with our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:56:24.231301Z",
     "start_time": "2023-04-21T14:56:22.023703Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(criterion='entropy', n_estimators=1000, oob_score=True, n_jobs=8)#, class_weight='balanced')\n",
    "y_pred = rfc.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Out-of-bag score:\", rfc.oob_score_)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"f1-score:\", metrics.f1_score(y_test, y_pred, pos_label=1))\n",
    "print(\"Feature importance:\", dict(zip(list(X_train), rfc.feature_importances_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:56:28.376114Z",
     "start_time": "2023-04-21T14:56:28.295065Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics.confusion_matrix(y_test, y_pred), [\"star\", \"QSO\"], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:56:36.073096Z",
     "start_time": "2023-04-21T14:56:34.610870Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "xx, yy = make_meshgrid(data[\"u-g\"], data[\"g-r\"])\n",
    "plot_contours(ax, rfc, xx, yy, 1,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.1)\n",
    "ax.scatter(data[\"u-g\"], data[\"g-r\"], c=(data.cat=='QSO'), cmap=plt.cm.coolwarm, s=20, edgecolors=None, alpha = 0.3)\n",
    "ax.set_xlabel(\"u-g\", fontsize=16)\n",
    "ax.set_ylabel(\"g-r\", fontsize=16)\n",
    "#ax.set_xlim(-2, 6)\n",
    "#ax.set_ylim(-2, 6)\n",
    "plt.savefig(\"boundary_non_weighted.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "In **boosting** the samples are weighted by the misclassification error, emphasizing the most difficult cases.\n",
    "\n",
    "It is motivated by the idea that combining many weak classifiers can result in improved classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than creating models separately on different data sets, boosting creates each model to attempt to correct the errors of the ensemble\n",
    "\n",
    "The central idea is to reweight the data based on how incorrectly the data were classified in the previous iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we had a weak classifier, $h(x)$, that we wish to apply to a data set and we want to create a strong classifier, $f(x)$, such that:\n",
    "\n",
    "$\\Large f(x) = \\sum\\limits_m^K \\theta_m h_m(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we start with a set of data, $X$, with known classifications, $y$, we can assign a weight, $w_m(x)$ to each point.\n",
    "\n",
    "After the application of the weak classifier, $h_m(x)$, we can estimate the classification error, $e_m$, as:\n",
    "\n",
    "$\\Large e_m = \\sum\\limits_{i=1}^N w_m(x_i) I(h_m(x_i) \\ne y_i)$\n",
    "\n",
    "where $I(h_m(x_i) \\ne y_i)$ is 1 for $h_m(x_i) \\ne y_i$ and 0 otherwise.\n",
    "\n",
    "From this we define the weight $\\theta_m$:\n",
    "\n",
    "$\\Large \\theta_m = \\frac{1}{2} \\log \\biggl(\\frac{1 - e_m}{e_m} \\biggr)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and update the weights for each point:\n",
    "\n",
    "$\\Large w_{m+1}(x_i) = w_m(x_i) ~\\times ~ \\biggr\\lbrace\\begin{array}{lr}\n",
    "\\exp(-\\theta_m) & h_m(x_i) = y_i \\\\\n",
    "\\exp(\\theta_m) & h_m(x_i) \\ne y_i\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is known as **AdaBoost**, see https://en.wikipedia.org/wiki/AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with boosted decision trees is that since they run serially they cannot be easily parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:56:50.573270Z",
     "start_time": "2023-04-21T14:56:50.429070Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc = AdaBoostClassifier()\n",
    "y_pred = abc.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"f1-score:\", metrics.f1_score(y_test, y_pred, pos_label=1))\n",
    "print(\"Feature importance:\", dict(zip(list(X_train), abc.feature_importances_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:56:51.994282Z",
     "start_time": "2023-04-21T14:56:51.786748Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics.confusion_matrix(y_test, y_pred), [\"star\", \"QSO\"], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:56:58.595646Z",
     "start_time": "2023-04-21T14:56:57.836097Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "xx, yy = make_meshgrid(data[\"u-g\"], data[\"g-r\"])\n",
    "plot_contours(ax, abc, xx, yy, 1,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.1)\n",
    "ax.scatter(data[\"u-g\"], data[\"g-r\"], c=(data.cat=='QSO'), cmap=plt.cm.coolwarm, s=20, edgecolors=None, alpha = 0.3)\n",
    "ax.set_xlabel(\"u-g\", fontsize=16)\n",
    "ax.set_ylabel(\"g-r\", fontsize=16)\n",
    "ax.set_xlim(-2, 6)\n",
    "ax.set_ylim(-2, 6)\n",
    "plt.savefig(\"boundary_non_weighted.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a variation from AdaBoost. \n",
    "\n",
    "In this type of classifier we try to approximate a steepest descent criterion after each evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is to assume that the $m+1$ classifier gives the correct classifications after applying a correction $h$ to the $m$ classifier:\n",
    "\n",
    "$\\Large F_{m+1}(x) = F_m(x) + h(x) = y$\n",
    "\n",
    "where $F_m(x)$ is the $m$-th classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that residual $h(x)$:\n",
    "\n",
    "$\\Large h(x) = y - F_m(x)$\n",
    "\n",
    "The name follows from the observation that the residuals $h_m(x)$ for a given model are the negative gradients of the mean squared error loss functions w.r.t. F(x):\n",
    "\n",
    "$\\Large L_{MSE} = \\frac{1}{2} (y - F_m(x))^2$\n",
    "\n",
    "$\\Large h_m(x) = \\frac{- \\partial L_{MSE}}{\\partial F} = y - F(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:57:04.923313Z",
     "start_time": "2023-04-21T14:57:04.635739Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "y_pred = gbc.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"f1-score:\", metrics.f1_score(y_test, y_pred, pos_label=1))\n",
    "print(\"Feature importance:\", dict(zip(list(X_train), abc.feature_importances_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:57:06.893187Z",
     "start_time": "2023-04-21T14:57:06.811254Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics.confusion_matrix(y_test, y_pred), [\"star\", \"QSO\"], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:57:17.215734Z",
     "start_time": "2023-04-21T14:57:16.831593Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "xx, yy = make_meshgrid(data[\"u-g\"], data[\"g-r\"])\n",
    "plot_contours(ax, gbc, xx, yy, 1,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.1)\n",
    "ax.scatter(data[\"u-g\"], data[\"g-r\"], c=(data.cat=='QSO'), cmap=plt.cm.coolwarm, s=20, edgecolors=None, alpha = 0.3)\n",
    "ax.set_xlabel(\"u-g\", fontsize=16)\n",
    "ax.set_ylabel(\"g-r\", fontsize=16)\n",
    "ax.set_xlim(-2, 6)\n",
    "ax.set_ylim(-2, 6)\n",
    "plt.savefig(\"boundary_non_weighted.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn provides some useful tools for choosing parameters via cross-validation in `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:57:20.151836Z",
     "start_time": "2023-04-21T14:57:20.149476Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:57:29.306739Z",
     "start_time": "2023-04-21T14:57:21.279282Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {'criterion': ('gini', 'entropy'), \n",
    "              'n_estimators': (5, 10, 20, 100),\n",
    "              'max_depth': (20, 40)}\n",
    "rfc = RandomForestClassifier(n_jobs=1, class_weight='balanced')\n",
    "clf = GridSearchCV(rfc, parameters, cv=5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:58:02.754441Z",
     "start_time": "2023-04-21T14:58:02.750014Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:58:04.417590Z",
     "start_time": "2023-04-21T14:58:04.414344Z"
    }
   },
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:58:09.328177Z",
     "start_time": "2023-04-21T14:58:09.290732Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"f1-score:\", metrics.f1_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Gradient Boosting (xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most popular and successful supervised classification algorithms.\n",
    "\n",
    "https://xgboost.ai/\n",
    "\n",
    "Focus on speed and model performance. \n",
    "\n",
    "![](images/xgboost.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost has been used by several teams wining Kaggle challenges.\n",
    "\n",
    "One of its focus is speed:\n",
    "\n",
    "- parallel tree computation\n",
    "- tree pruning on the fly\n",
    "- hardware optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/xgboost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the algorithm, the following changes have been made:\n",
    "\n",
    "- regularization via LASSO and Ridge regularization\n",
    "- it accepts sparse data with missing values, by learning best missing values\n",
    "- it uses **quantile sketch algorithm** to find optimal split points\n",
    "- built-in cross-validation at each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/xgboost_performance.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light Gradient Boosting Microsoft (lightGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fast, distributed, high-preformance gradient boosting, developed by Microsoft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LightGBM grows trees horizontally (leaf-wise) while other algorithms grow level-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/lightgbm_comp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform labels to numbers with LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:58:15.400454Z",
     "start_time": "2023-04-21T14:58:15.397454Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T14:58:20.899705Z",
     "start_time": "2023-04-21T14:58:20.897026Z"
    }
   },
   "outputs": [],
   "source": [
    "yt_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:03:26.201662Z",
     "start_time": "2023-04-21T15:03:26.198351Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_contours_xgb(ax, xg, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = xg.predict(xgb.DMatrix(pd.DataFrame(data = np.c_[xx.ravel(), yy.ravel()], columns=[\"u-g\", \"g-r\"]))) >= 0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:03:26.690415Z",
     "start_time": "2023-04-21T15:03:26.688281Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:03:27.208964Z",
     "start_time": "2023-04-21T15:03:27.202498Z"
    }
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train,label=yt_train)\n",
    "dtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:03:28.242455Z",
     "start_time": "2023-04-21T15:03:28.239341Z"
    }
   },
   "outputs": [],
   "source": [
    "#setting parameters for xgboost\n",
    "parameters={'max_depth':7, 'eta':1, 'silent':1,'objective':'binary:logistic','eval_metric':'auc','learning_rate':.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:03:29.034347Z",
     "start_time": "2023-04-21T15:03:28.816498Z"
    }
   },
   "outputs": [],
   "source": [
    "num_round = 50 \n",
    "xg = xgb.train(parameters,dtrain,num_round) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:03:29.779991Z",
     "start_time": "2023-04-21T15:03:29.773252Z"
    }
   },
   "outputs": [],
   "source": [
    "xg.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:03:30.377085Z",
     "start_time": "2023-04-21T15:03:30.369575Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = np.array(xg.predict(dtest) >= 0.5, dtype=int)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:04:29.420379Z",
     "start_time": "2023-04-21T15:04:29.337110Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics.confusion_matrix(np.array(yt_test), y_pred, le.transform([\"QSO\", \"star\"])), [\"QSO\", \"star\"], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:04:34.633826Z",
     "start_time": "2023-04-21T15:04:34.196801Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "xx, yy = make_meshgrid(data[\"u-g\"], data[\"g-r\"])\n",
    "plot_contours_xgb(ax, xg, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.1)\n",
    "ax.scatter(data[\"u-g\"], data[\"g-r\"], c=(data.cat=='QSO'), cmap=plt.cm.coolwarm, s=20, edgecolors=None, alpha = 0.3)\n",
    "ax.set_xlabel(\"u-g\", fontsize=16)\n",
    "ax.set_ylabel(\"g-r\", fontsize=16)\n",
    "ax.set_xlim(-2, 6)\n",
    "ax.set_ylim(-2, 6)\n",
    "plt.savefig(\"boundary_non_weighted.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:04:36.828955Z",
     "start_time": "2023-04-21T15:04:36.824643Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_contours_lgm(ax, bst, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = bst.predict(np.c_[xx.ravel(), yy.ravel()]) >= 0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:04:49.668524Z",
     "start_time": "2023-04-21T15:04:49.631970Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:04:50.271552Z",
     "start_time": "2023-04-21T15:04:50.268782Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_train, label=yt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:04:51.057458Z",
     "start_time": "2023-04-21T15:04:51.053743Z"
    }
   },
   "outputs": [],
   "source": [
    "param = param = {'num_leaves':150, 'objective':'binary','max_depth':7,'learning_rate':.05,'max_bin':200, 'is_unbalanced':True}\n",
    "param['metric'] = 'auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:04:51.526330Z",
     "start_time": "2023-04-21T15:04:51.483660Z"
    }
   },
   "outputs": [],
   "source": [
    "num_round = 10\n",
    "bst = lgb.train(param, train_data, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:04:51.894316Z",
     "start_time": "2023-04-21T15:04:51.722002Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb.cv(param, train_data, num_round, nfold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:04:58.329143Z",
     "start_time": "2023-04-21T15:04:58.326121Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = np.array(bst.predict(X_test) >= 0.5, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:04:58.992953Z",
     "start_time": "2023-04-21T15:04:58.899516Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics.confusion_matrix(np.array(yt_test), y_pred, le.transform([\"QSO\", \"star\"])), [\"QSO\", \"star\"], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:05:00.763930Z",
     "start_time": "2023-04-21T15:05:00.372577Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "xx, yy = make_meshgrid(data[\"u-g\"], data[\"g-r\"])\n",
    "plot_contours_lgm(ax, bst, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.1)\n",
    "ax.scatter(data[\"u-g\"], data[\"g-r\"], c=(data.cat=='QSO'), cmap=plt.cm.coolwarm, s=20, edgecolors=None, alpha = 0.3)\n",
    "ax.set_xlabel(\"u-g\", fontsize=16)\n",
    "ax.set_ylabel(\"g-r\", fontsize=16)\n",
    "ax.set_xlim(-2, 6)\n",
    "ax.set_ylim(-2, 6)\n",
    "plt.savefig(\"boundary_non_weighted.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/summary_Ivezic1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/summary_Ivezic2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/summary_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/imbdata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:05:06.819588Z",
     "start_time": "2023-04-21T15:05:06.800398Z"
    }
   },
   "outputs": [],
   "source": [
    "sel_cols = [\"u-g\", \"g-r\", \"r-i\", \"i-z\", \"cat\"]\n",
    "data = pd.concat([stars[sel_cols], QSOs[sel_cols]])\n",
    "data[\"cat\"] = data[\"cat\"].astype(\"category\")\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:05:07.517276Z",
     "start_time": "2023-04-21T15:05:07.498342Z"
    }
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:05:08.319281Z",
     "start_time": "2023-04-21T15:05:08.313465Z"
    }
   },
   "outputs": [],
   "source": [
    "data.groupby(\"cat\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:05:08.778461Z",
     "start_time": "2023-04-21T15:05:08.760964Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[[\"u-g\", \"g-r\"]], data.cat, test_size=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:05:09.391812Z",
     "start_time": "2023-04-21T15:05:09.373867Z"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:05:10.294191Z",
     "start_time": "2023-04-21T15:05:09.843733Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = {\"QSO\": 'r', \"star\": 'b'}\n",
    "for cl in colors.keys():\n",
    "    ax.scatter(data.loc[data.cat == cl][\"u-g\"], data.loc[data.cat == cl][\"g-r\"], marker='.', alpha=0.5, label=cl, color=colors[cl])\n",
    "ax.set_xlabel(\"u-g\", fontsize=18)\n",
    "ax.set_ylabel(\"g-r\", fontsize=18)\n",
    "ax.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE: Synthetic Minority Over-sampling Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://jair.org/index.php/jair/article/view/10302/24590"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**\n",
    "\n",
    "An approach to the construction of classifiers from imbalanced datasets is described.A dataset is imbalanced if the classification categories are not approximately equally represented.  Often real-world data sets are predominately composed of “normal” examples with only a small percentage of “abnormal” or “interesting” examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://imbalanced-learn.readthedocs.io/en/stable/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/SMOTE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adaptively shifts the classification decision boundary toward the difficult examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, ANS, Border SMOTE, Safe Level SMOTE, DBSMOTE, SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:05:13.403391Z",
     "start_time": "2023-04-21T15:05:13.369296Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:05:32.923241Z",
     "start_time": "2023-04-21T15:05:32.837265Z"
    }
   },
   "outputs": [],
   "source": [
    "#X_resampled, y_resampled = SMOTE().fit_resample(X_train, y_train)\n",
    "X_resampled, y_resampled = ADASYN().fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:05:45.484605Z",
     "start_time": "2023-04-21T15:05:45.477076Z"
    }
   },
   "outputs": [],
   "source": [
    "X_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:05:50.186654Z",
     "start_time": "2023-04-21T15:05:50.179929Z"
    }
   },
   "outputs": [],
   "source": [
    "y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:05:33.346752Z",
     "start_time": "2023-04-21T15:05:33.321821Z"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(y_resampled, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:06:20.742001Z",
     "start_time": "2023-04-21T15:06:20.731859Z"
    }
   },
   "outputs": [],
   "source": [
    "data_resampled = pd.DataFrame()\n",
    "data_resampled[\"u-g\"] = X_resampled[\"u-g\"]\n",
    "data_resampled[\"g-r\"] = X_resampled[\"g-r\"]\n",
    "data_resampled[\"cat\"] = y_resampled\n",
    "data_resampled[\"cat\"] = data_resampled[\"cat\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:06:22.666819Z",
     "start_time": "2023-04-21T15:06:22.215348Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = {\"QSO\": 'r', \"star\": 'b'}\n",
    "for cl in colors.keys():\n",
    "    ax.scatter(data.loc[data.cat == cl][\"u-g\"], data.loc[data.cat == cl][\"g-r\"], marker='.', alpha=0.5, label=cl, color=colors[cl])\n",
    "ax.set_xlabel(\"u-g\", fontsize=18)\n",
    "ax.set_ylabel(\"g-r\", fontsize=18)\n",
    "ax.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:06:31.888562Z",
     "start_time": "2023-04-21T15:06:31.534977Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = {\"QSO\": 'r', \"star\": 'b'}\n",
    "for cl in colors.keys():\n",
    "    ax.scatter(data_resampled.loc[data_resampled.cat == cl][\"u-g\"], data_resampled.loc[data_resampled.cat == cl][\"g-r\"], marker='.', alpha=0.5, label=cl, color=colors[cl])\n",
    "ax.set_xlabel(\"u-g\", fontsize=18)\n",
    "ax.set_ylabel(\"g-r\", fontsize=18)\n",
    "ax.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:06:39.458655Z",
     "start_time": "2023-04-21T15:06:39.067553Z"
    }
   },
   "outputs": [],
   "source": [
    "points = hv.Points(data_resampled, kdims=['u-g', 'g-r'], vdims=['cat'])\n",
    "datashade.x_range=(data_resampled[\"u-g\"].quantile(0.001),data_resampled[\"u-g\"].quantile(0.995))\n",
    "datashade.y_range=(data_resampled[\"g-r\"].quantile(0.001),data_resampled[\"g-r\"].quantile(0.995)); \n",
    "datashade(points, aggregator=ds.count_cat('cat')).opts(width=800, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM using the resampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:06:57.070245Z",
     "start_time": "2023-04-21T15:06:57.066903Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_contours_lgm(ax, bst, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = bst.predict(np.c_[xx.ravel(), yy.ravel()]) >= 0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:06:57.861240Z",
     "start_time": "2023-04-21T15:06:57.858875Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:06:59.375863Z",
     "start_time": "2023-04-21T15:06:59.357780Z"
    }
   },
   "outputs": [],
   "source": [
    "yt_test = le.transform(y_test)\n",
    "yt_train = le.transform(y_train)\n",
    "yt_resampled = le.transform(y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lgb dataset objects with and without resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:07:00.378317Z",
     "start_time": "2023-04-21T15:07:00.375319Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_train, label=yt_train)\n",
    "train_data_resampled = lgb.Dataset(X_resampled, label=yt_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:07:00.851835Z",
     "start_time": "2023-04-21T15:07:00.849709Z"
    }
   },
   "outputs": [],
   "source": [
    "param = param = {'num_leaves':150, 'objective':'binary','max_depth':7,'learning_rate':.05,'max_bin':200, 'is_unbalanced':False}\n",
    "param['metric'] = 'auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:07:01.581746Z",
     "start_time": "2023-04-21T15:07:01.417260Z"
    }
   },
   "outputs": [],
   "source": [
    "num_round = 10\n",
    "bst = lgb.train(param, train_data, num_round)\n",
    "bst_resampled = lgb.train(param, train_data_resampled, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:07:04.171045Z",
     "start_time": "2023-04-21T15:07:03.975101Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb.cv(param, train_data, num_round, nfold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:07:04.815002Z",
     "start_time": "2023-04-21T15:07:04.458239Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb.cv(param, train_data_resampled, num_round, nfold=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict classes with and without resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:07:06.527914Z",
     "start_time": "2023-04-21T15:07:06.515364Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = np.array(bst.predict(X_test) >= 0.5, dtype=int)\n",
    "y_pred_resampled = np.array(bst_resampled.predict(X_test) >= 0.5, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix without resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:07:09.703314Z",
     "start_time": "2023-04-21T15:07:09.611098Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics.confusion_matrix(np.array(yt_test), y_pred, le.transform([\"QSO\", \"star\"])), [\"QSO\", \"star\"], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:07:11.440972Z",
     "start_time": "2023-04-21T15:07:10.609168Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "xx, yy = make_meshgrid(data[\"u-g\"], data[\"g-r\"])\n",
    "plot_contours_lgm(ax, bst, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.1)\n",
    "ax.scatter(data[\"u-g\"], data[\"g-r\"], c=(data.cat=='QSO'), cmap=plt.cm.coolwarm, s=20, edgecolors=None, alpha = 0.01)\n",
    "ax.set_xlabel(\"u-g\", fontsize=16)\n",
    "ax.set_ylabel(\"g-r\", fontsize=16)\n",
    "ax.set_xlim(-2, 6)\n",
    "ax.set_ylim(-2, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix with resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:07:36.455144Z",
     "start_time": "2023-04-21T15:07:36.217557Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics.confusion_matrix(np.array(yt_test), y_pred_resampled, le.transform([\"QSO\", \"star\"])), [\"QSO\", \"star\"], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-21T15:07:38.056894Z",
     "start_time": "2023-04-21T15:07:37.224433Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12, 10))\n",
    "xx, yy = make_meshgrid(data_resampled[\"u-g\"], data_resampled[\"g-r\"])\n",
    "plot_contours_lgm(ax, bst_resampled, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.1)\n",
    "ax.scatter(data_resampled[\"u-g\"], data_resampled[\"g-r\"], c=(data_resampled.cat=='QSO'), cmap=plt.cm.coolwarm, s=20, edgecolors=None, alpha = 0.01)\n",
    "ax.set_xlabel(\"u-g\", fontsize=16)\n",
    "ax.set_ylabel(\"g-r\", fontsize=16)\n",
    "ax.set_xlim(-2, 6)\n",
    "ax.set_ylim(-2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "300px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

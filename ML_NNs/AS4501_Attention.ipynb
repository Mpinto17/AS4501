{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2270db60",
   "metadata": {},
   "source": [
    "$\\Huge AS4501$\n",
    "\n",
    "Transformers and Attention\n",
    "\n",
    "Francisco FÃ¶rster\n",
    "\n",
    "Bibliography:\n",
    "\n",
    "* [Attention is all you need, Vaswani et al. 2017](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "* https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html (many figures from this great website)\n",
    "* https://towardsdatascience.com/attention-and-transformer-models-fe667f958378"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8fec16",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21074456",
   "metadata": {},
   "source": [
    "Recurrent neural networks have two big problems:\n",
    "\n",
    "1. They tend to give too much weight to recent elements in a sequence, but sometimes the most important connections in a sentence are separated by a large number of elements.\n",
    "\n",
    "2. They are intrinsically serial in nature. We need to process a sequence in order to compute the output of a RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159403d3",
   "metadata": {},
   "source": [
    "This is how a RNN processes a sentence, paying more attention to the last word at each step and requiring a serial processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c14a9",
   "metadata": {},
   "source": [
    "![](images/sentence-classification-rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1163acd7",
   "metadata": {},
   "source": [
    "But in many cases the last word is not the most important, and we would like to be able to process each word and its association with other words in parallel:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e6bcf",
   "metadata": {},
   "source": [
    "![](images/sentence-example-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca6345",
   "metadata": {},
   "source": [
    "This also happens in the problem of translation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc00b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T03:26:40.740198Z",
     "start_time": "2023-05-27T03:26:40.629069Z"
    }
   },
   "source": [
    "![](images/sentence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17541e4",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d353c",
   "metadata": {},
   "source": [
    "Let's remember the softmax function applied to a vector x:\n",
    "\n",
    "$\\Large {\\rm softmax(x_i)} = \\frac{\\exp{x_i}}{\\sum\\limits_j \\exp{x_j}}$ \n",
    "\n",
    "This function returns ~1 at the largest value of the vector and ~0 elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea07efb8",
   "metadata": {},
   "source": [
    "![](images/softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da897d4",
   "metadata": {},
   "source": [
    "# Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c003461",
   "metadata": {},
   "source": [
    "The attention mechanism is an approach in deep learning that allows models to focus on different parts of the input when producing the output. Instead of focusing in some hidden state like in RNNs, in attention each output explicitly depends on all previous input states, weighted by attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cca037",
   "metadata": {},
   "source": [
    "For example in this sentence with the following attention scores:\n",
    "\n",
    " I love travelling\n",
    "   \n",
    "   [0.1,  0.2,  0.7] ---> J'adore\n",
    "  \n",
    "  [0.5,  0.5,  0.0] ---> voyager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3357e895",
   "metadata": {},
   "source": [
    "'J'adore' pays more attention or has more affinity to 'travelling' as the next word when translating.\n",
    "\n",
    "'voyager' pays attention to 'I' and 'love' equally when translating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959d13a",
   "metadata": {},
   "source": [
    "# Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a38367",
   "metadata": {},
   "source": [
    "Self Attention, also known as intra Attention, is an attention mechanism that relates different positions of one sequence in order to compute a representation of the same sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec836ef",
   "metadata": {},
   "source": [
    "![](images/intraattention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9744b86",
   "metadata": {},
   "source": [
    "In a self-attention layer, an input matrix $X$ ($n$ tokens of dimension $d$) are turned it into an output matrix $Z$ ($n$ components of dimension $d_v$) via three representational matrices of the input:\n",
    "\n",
    "* queries Q\n",
    "* keys K\n",
    "* values V\n",
    "\n",
    "$\\Large {\\rm Attention}(Q, K, V) = {\\rm softmax}( Q \\cdot K^T / \\sqrt{d_k}) * V$\n",
    "\n",
    "where $Q$, $K$ and $V$ are matrices representing linear transformations from the input vector $x$ via learnable parameters $W^Q$, $W^K$ and $W^V$:\n",
    "\n",
    "* $Q = X W^Q$\n",
    "* $K = X W^K$\n",
    "* $V = X W^V$\n",
    "\n",
    "Note that \n",
    "* $x \\in \\mathbb{R}^{n \\times d}$\n",
    "* $Q \\in \\mathbb{R}^{n \\times d_k}$\n",
    "* $K \\in \\mathbb{R}^{n \\times d_k}$\n",
    "* $V \\in \\mathbb{R}^{n \\times d_v}$\n",
    "* $W^Q \\in \\mathbb{R}^{d_k \\times d}$\n",
    "* $W^K \\in \\mathbb{R}^{d_k \\times d}$\n",
    "* $W^V \\in \\mathbb{R}^{d_v \\times d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576fa1d",
   "metadata": {},
   "source": [
    "![](images/attention_detail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb2033",
   "metadata": {},
   "source": [
    "![](images/selfattention_summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ac3b1",
   "metadata": {},
   "source": [
    "# Cross-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11692cd",
   "metadata": {},
   "source": [
    "One can generalize the previous computation for combining two input matrices $X_1$ and $X_2$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5325723d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T02:13:31.845022Z",
     "start_time": "2023-06-02T02:13:31.724947Z"
    }
   },
   "source": [
    "![](images/cross-attention-summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0293866",
   "metadata": {},
   "source": [
    "And this is an example of a cross attention matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a499b",
   "metadata": {},
   "source": [
    "![](images/bahdanau-fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e27a8d",
   "metadata": {},
   "source": [
    "and a visualization of one row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482beb54",
   "metadata": {},
   "source": [
    "![](images/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca7086",
   "metadata": {},
   "source": [
    "# Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f37112",
   "metadata": {},
   "source": [
    "In multi-head attention we concatenate the output from several heads $i$ with learnable parameters $W_i^Q$, $W_i^K$ and $W_i^V$, and then linearly transform this vector with learnable parameters $W^O$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac31057",
   "metadata": {},
   "source": [
    "$\\Large {\\rm Multihead} = {\\rm concat}({\\rm head}_1, ... {\\rm head}_h) W^O$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d054c188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T02:23:57.613635Z",
     "start_time": "2023-06-02T02:23:57.491341Z"
    }
   },
   "source": [
    "![](images/multi-head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78933c46",
   "metadata": {},
   "source": [
    "# Positional encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0a446",
   "metadata": {},
   "source": [
    "One problem with the previous strategy is that the order of the input is never used to compute the attention scores. In order to fix this problem, information about the relative positions of the inputs must be added. In the original paper by Vaswani they use sine and cosine functions of different frequencies:\n",
    "\n",
    "* $PE(pos, 2i) = sin(pos / 10000^{2i/d})$\n",
    "* $PE(pos, 2i) = cos(pos / 10000^{2i/d})$\n",
    "\n",
    "![](images/PE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab936cce",
   "metadata": {},
   "source": [
    "In other works, a set of functions are learned as the positional encoder. For example, in [Pimentel+2023](https://arxiv.org/pdf/2201.08482.pdf) they use the following function (timeFiLM):\n",
    "\n",
    "![](images/timefilm.png)\n",
    "![](images/timefilm2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d009ed",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b030c",
   "metadata": {},
   "source": [
    "The full transformer arquitecture proposed by Vaswani et al. 2017 is the following:\n",
    "\n",
    "![](images/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba289a",
   "metadata": {},
   "source": [
    "The model is composed of an encoder and a decoder. \n",
    "\n",
    "The encoder is composed of 6 identical layers, each one with two sublayers: a multi-head self-attention mechanism and a position wise fully connected feed-forward network. The output of each sublayer uses a residual connection (we add the input to the output of the sublayer), which helps with convergence, and is normalized using layer normalization.\n",
    "\n",
    "The decoder is also composed of 6 identical layers. In addition to the two sublayers used in the encoder, a sublayer is added in between that uses multihead cross attention with the output of the encoder. The multihead self-attention is also modified to mask positions that have not been visited by the decoder (predictions for position i can depend only on the known outputs of positions less than i).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be9015",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66547a",
   "metadata": {},
   "source": [
    "## Internet movie database reviews using [BERT](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73fbd36",
   "metadata": {},
   "source": [
    "![](images/bert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b8da2",
   "metadata": {},
   "source": [
    "Here we will load the weights and biases of from [BERT](https://arxiv.org/abs/1810.04805) and will fine tune it to reproduce reviews from [IMDB datasets](https://www.tensorflow.org/datasets/catalog/imdb_reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab2f5cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:39:32.252769Z",
     "start_time": "2023-06-02T15:39:29.523363Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from transformers import BertTokenizerFast, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56343242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:39:40.672556Z",
     "start_time": "2023-06-02T15:39:40.570796Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load the IMDB dataset\n",
    "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc6eb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:39:44.744469Z",
     "start_time": "2023-06-02T15:39:40.673780Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert datasets to NumPy arrays and get labels\n",
    "train_reviews = []\n",
    "train_labels = []\n",
    "for review, label in tfds.as_numpy(train_dataset):\n",
    "    train_reviews.append(review)\n",
    "    train_labels.append(label)\n",
    "\n",
    "test_reviews = []\n",
    "test_labels = []\n",
    "for review, label in tfds.as_numpy(test_dataset):\n",
    "    test_reviews.append(review)\n",
    "    test_labels.append(label)\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab8dca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:39:44.777942Z",
     "start_time": "2023-06-02T15:39:44.745679Z"
    }
   },
   "outputs": [],
   "source": [
    "train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf0fb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:39:44.781482Z",
     "start_time": "2023-06-02T15:39:44.779249Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe9110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:39:45.008929Z",
     "start_time": "2023-06-02T15:39:44.782562Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Initialize the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91f041b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:40:11.576747Z",
     "start_time": "2023-06-02T15:39:45.010234Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Preprocess the training and testing data\n",
    "def encode_reviews(tokenizer, reviews, max_length):\n",
    "    token_ids = np.zeros(shape=(len(reviews), max_length), dtype=np.int32)\n",
    "    for i, review in enumerate(reviews):\n",
    "        encoded = tokenizer.encode(review.decode(), max_length=max_length)\n",
    "        token_ids[i, :len(encoded)] = encoded\n",
    "    attention_mask = (token_ids != 0).astype(np.int32)\n",
    "    return {\"input_ids\": token_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "# Define a max_length for the reviews\n",
    "max_length = 512  # Adjust this depending on your resources\n",
    "\n",
    "# Preprocess the training data\n",
    "train_data = encode_reviews(tokenizer, train_reviews, max_length)\n",
    "\n",
    "# Preprocess the testing data\n",
    "test_data = encode_reviews(tokenizer, test_reviews, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbaccb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:40:13.203820Z",
     "start_time": "2023-06-02T15:40:11.577936Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Initialize the model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0174bee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:40:13.214532Z",
     "start_time": "2023-06-02T15:40:13.206053Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9269510",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:40:20.358676Z",
     "start_time": "2023-06-02T15:40:13.216179Z"
    }
   },
   "outputs": [],
   "source": [
    "# 6. Train the model\n",
    "model.fit(train_data, train_labels, validation_data=(test_data, test_labels), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e45fc5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:40:20.361978Z",
     "start_time": "2023-06-02T15:39:40.584Z"
    }
   },
   "outputs": [],
   "source": [
    "# 7. Evaluation\n",
    "# Evaluate the model on the test set\n",
    "model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a41e85",
   "metadata": {},
   "source": [
    "## Vision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95479e7",
   "metadata": {},
   "source": [
    "This is based on the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n",
    "](https://arxiv.org/abs/2010.11929)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a61305",
   "metadata": {},
   "source": [
    "![](images/vit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c63822",
   "metadata": {},
   "source": [
    "See https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba84a40",
   "metadata": {},
   "source": [
    "We will use the Huggingface library to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bbebff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:05:37.046299Z",
     "start_time": "2023-06-02T16:05:37.017190Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2069b3c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:05:45.031151Z",
     "start_time": "2023-06-02T16:05:44.639014Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090811a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:05:45.866728Z",
     "start_time": "2023-06-02T16:05:45.864986Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"microsoft/swin-tiny-patch4-window7-224\" # pre-trained model from which to fine-tune\n",
    "batch_size = 32 # batch size for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e3a997",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:05:48.547808Z",
     "start_time": "2023-06-02T16:05:46.695977Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd333d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:05:50.241870Z",
     "start_time": "2023-06-02T16:05:49.393921Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c43f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:05:51.077300Z",
     "start_time": "2023-06-02T16:05:51.074838Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd155cc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:05:51.935468Z",
     "start_time": "2023-06-02T16:05:51.929059Z"
    }
   },
   "outputs": [],
   "source": [
    "example = dataset[\"train\"][10]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959f4c3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:05:52.775808Z",
     "start_time": "2023-06-02T16:05:52.773411Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8368c75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:05:53.621848Z",
     "start_time": "2023-06-02T16:05:53.618864Z"
    }
   },
   "outputs": [],
   "source": [
    "example['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472ee20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:05:54.480072Z",
     "start_time": "2023-06-02T16:05:54.468803Z"
    }
   },
   "outputs": [],
   "source": [
    "example['img'].resize((200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83234fdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:05:55.443152Z",
     "start_time": "2023-06-02T16:05:55.440018Z"
    }
   },
   "outputs": [],
   "source": [
    "example['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cdcaa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:05:56.337590Z",
     "start_time": "2023-06-02T16:05:56.334758Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"].features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4337143",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:05:57.178506Z",
     "start_time": "2023-06-02T16:05:57.174914Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = dataset[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "id2label[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b7fcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:05:58.247390Z",
     "start_time": "2023-06-02T16:05:58.036164Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor  = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
    "image_processor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78a31d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:06:06.437246Z",
     "start_time": "2023-06-02T16:06:06.374946Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "if \"height\" in image_processor.size:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = image_processor.size.get(\"longest_edge\")\n",
    "\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(crop_size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"img\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"img\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadad995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:06:07.479586Z",
     "start_time": "2023-06-02T16:06:07.458487Z"
    }
   },
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "splits = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e2147b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:06:08.333464Z",
     "start_time": "2023-06-02T16:06:08.326783Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be536431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:06:09.210950Z",
     "start_time": "2023-06-02T16:06:09.181717Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d187598",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:06:10.531740Z",
     "start_time": "2023-06-02T16:06:10.048042Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb924ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:06:11.382802Z",
     "start_time": "2023-06-02T16:06:11.378305Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-eurosat\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f8e7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:06:12.451639Z",
     "start_time": "2023-06-02T16:06:12.447071Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9882f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:06:13.541838Z",
     "start_time": "2023-06-02T16:06:13.536758Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ccb78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T16:06:15.657220Z",
     "start_time": "2023-06-02T16:06:14.934012Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()\n",
    "# rest is optional but nice to have\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3246015",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T15:55:39.069046Z",
     "start_time": "2023-06-02T15:41:25.479Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "# some nice to haves:\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48916c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

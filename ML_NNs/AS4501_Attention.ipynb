{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2270db60",
   "metadata": {},
   "source": [
    "$\\Huge AS4501$\n",
    "\n",
    "Transformers and Attention\n",
    "\n",
    "Francisco FÃ¶rster\n",
    "\n",
    "Bibliography:\n",
    "\n",
    "* [Attention is all you need, Vaswani et al. 2017](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "* https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html (many figures from this great website)\n",
    "* https://towardsdatascience.com/attention-and-transformer-models-fe667f958378"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8fec16",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21074456",
   "metadata": {},
   "source": [
    "Recurrent neural networks have two big problems:\n",
    "\n",
    "1. They tend to give too much weight to recent elements in a sequence, but sometimes the most important connections in a sentence are separated by a large number of elements.\n",
    "\n",
    "2. They are intrinsically serial in nature. We need to process a sequence in order to compute the output of a RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159403d3",
   "metadata": {},
   "source": [
    "This is how a RNN processes a sentence, paying more attention to the last word at each step and requiring a serial processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c14a9",
   "metadata": {},
   "source": [
    "![](images/sentence-classification-rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1163acd7",
   "metadata": {},
   "source": [
    "But in many cases the last word is not the most important, and we would like to be able to process each word and its association with other words in parallel:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e6bcf",
   "metadata": {},
   "source": [
    "![](images/sentence-example-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca6345",
   "metadata": {},
   "source": [
    "This also happens in the problem of translation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc00b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T03:26:40.740198Z",
     "start_time": "2023-05-27T03:26:40.629069Z"
    }
   },
   "source": [
    "![](images/sentence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da897d4",
   "metadata": {},
   "source": [
    "# Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c003461",
   "metadata": {},
   "source": [
    "The attention mechanism is an approach in deep learning that allows models to focus on different parts of the input when producing the output. Instead of focusing in some hidden state like in RNNs, in attention each output explicitly depends on all previous input states, weighted by attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cca037",
   "metadata": {},
   "source": [
    "For example in this sentence with the following attention scores:\n",
    "\n",
    " I love travelling\n",
    "   \n",
    "   [0.1,  0.2,  0.7] ---> J'adore\n",
    "  \n",
    "  [0.5,  0.5,  0.0] ---> voyager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3357e895",
   "metadata": {},
   "source": [
    "'J'adore' pays more attention or has more affinity to 'travelling' when translating.\n",
    "\n",
    "'voyager' pays attention to 'I' and 'love' equally when translating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959d13a",
   "metadata": {},
   "source": [
    "# Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a38367",
   "metadata": {},
   "source": [
    "Self Attention, also known as intra Attention, is an attention mechanism that relates different positions of one sequence in order to compute a representation of the same sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec836ef",
   "metadata": {},
   "source": [
    "![](images/intraattention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d353c",
   "metadata": {},
   "source": [
    "Let's remember the softmax function applied to a vector x:\n",
    "\n",
    "$\\Large {\\rm softmax(x_i)} = \\frac{\\exp{x_i}}{\\sum\\limits_j \\exp{x_j}}$ \n",
    "\n",
    "This function returns ~1 at the largest value of the vector and ~0 elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea07efb8",
   "metadata": {},
   "source": [
    "![](images/softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9744b86",
   "metadata": {},
   "source": [
    "In a self-attention layer, an input matrix $X$ ($n$ tokens of dimension $d$) are turned it into an output matrix $Z$ ($n$ components of dimension $d_v$) via three representational matrices of the input:\n",
    "\n",
    "* queries Q\n",
    "* keys K\n",
    "* values V\n",
    "\n",
    "$\\Large {\\rm Attention}(Q, K, V) = {\\rm softmax}( Q \\cdot K^T / \\sqrt{d_k}) * V$\n",
    "\n",
    "where $Q$, $K$ and $V$ are matrices representing linear transformations from the input vector $x$ via learnable parameters $W^Q$, $W^K$ and $W^V$:\n",
    "\n",
    "* $Q = X W^Q$\n",
    "* $K = X W^K$\n",
    "* $V = X W^V$\n",
    "\n",
    "Note that \n",
    "* $x \\in \\mathbb{R}^{n \\times d}$\n",
    "* $Q \\in \\mathbb{R}^{n \\times d_k}$\n",
    "* $K \\in \\mathbb{R}^{n \\times d_k}$\n",
    "* $V \\in \\mathbb{R}^{n \\times d_v}$\n",
    "* $W^Q \\in \\mathbb{R}^{d_k \\times d}$\n",
    "* $W^K \\in \\mathbb{R}^{d_k \\times d}$\n",
    "* $W^V \\in \\mathbb{R}^{d_v \\times d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576fa1d",
   "metadata": {},
   "source": [
    "![](images/attention_detail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb2033",
   "metadata": {},
   "source": [
    "![](images/selfattention_summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ac3b1",
   "metadata": {},
   "source": [
    "# Cross-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11692cd",
   "metadata": {},
   "source": [
    "One can generalize the previous computation for combining two input matrices $X_1$ and $X_2$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5325723d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T02:13:31.845022Z",
     "start_time": "2023-06-02T02:13:31.724947Z"
    }
   },
   "source": [
    "![](images/cross-attention-summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0293866",
   "metadata": {},
   "source": [
    "And this is an example of a cross attention matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a499b",
   "metadata": {},
   "source": [
    "![](images/bahdanau-fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e27a8d",
   "metadata": {},
   "source": [
    "and a visualization of one row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482beb54",
   "metadata": {},
   "source": [
    "![](images/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca7086",
   "metadata": {},
   "source": [
    "# Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f37112",
   "metadata": {},
   "source": [
    "In multi-head attention we concatenate the output from several heads $i$ with learnable parameters $W_i^Q$, $W_i^K$ and $W_i^V$, and then linearly transform this vector with learnable parameters $W^O$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac31057",
   "metadata": {},
   "source": [
    "$\\Large {\\rm Multihead} = {\\rm concat}({\\rm head}_1, ... {\\rm head}_h) W^O$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d054c188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T02:23:57.613635Z",
     "start_time": "2023-06-02T02:23:57.491341Z"
    }
   },
   "source": [
    "![](images/multi-head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78933c46",
   "metadata": {},
   "source": [
    "# Positional encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0a446",
   "metadata": {},
   "source": [
    "One problem with the previous strategy is that the order of the input is never used to compute the attention scores. In order to fix this problem, information about the relative positions of the inputs must be added. In the original paper by Vaswani they use sine and cosine functions of different frequencies:\n",
    "\n",
    "* $PE(pos, 2i) = sin(pos / 10000^{2i/d})$\n",
    "* $PE(pos, 2i) = cos(pos / 10000^{2i/d})$\n",
    "\n",
    "![](images/PE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab936cce",
   "metadata": {},
   "source": [
    "In other works, a set of functions are learned as the positional encoder. For example, in [Pimentel+2023](https://arxiv.org/pdf/2201.08482.pdf) they use the following function (timeFiLM):\n",
    "\n",
    "![](images/timefilm.png)\n",
    "![](images/timefilm2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d009ed",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b030c",
   "metadata": {},
   "source": [
    "The full transformer arquitecture proposed by Vaswani et al. 2017 is the following:\n",
    "\n",
    "![](images/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba289a",
   "metadata": {},
   "source": [
    "The model is composed of an encoder and a decoder. \n",
    "\n",
    "The encoder is composed of 6 identical layers, each one with two sublayers: a multi-head self-attention mechanism and a position wise fully connected feed-forward network. The output of each sublayer uses a residual connection (we add the input to the output of the sublayer), which helps with convergence, and is normalized using layer normalization.\n",
    "\n",
    "The decoder is also composed of 6 identical layers. In addition to the two sublayers used in the encoder, a sublayer is added in between that uses multihead cross attention with the output of the encoder. The multihead self-attention is also modified to mask positions that have not been visited by the decoder (predictions for position i can depend only on the known outputs of positions less than i).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0a24ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T03:33:59.074023Z",
     "start_time": "2023-06-02T03:33:55.271973Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow tensorflow_datasets transformers\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3cc0703",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T03:39:14.169850Z",
     "start_time": "2023-06-02T03:39:13.974641Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load the Dataset\n",
    "# Load the IMDB dataset\n",
    "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c84fd87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T03:39:16.307052Z",
     "start_time": "2023-06-02T03:39:15.293731Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Preprocessing\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a function to encode the texts\n",
    "def encode_texts(text, label):\n",
    "    encoded_text = tokenizer(text.numpy().decode('utf-8'), truncation=True, padding='max_length', max_length=512, return_tensors='tf')\n",
    "    return encoded_text['input_ids'], encoded_text['attention_mask'], label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "    # py_func doesn't set the shape of the returned tensors.\n",
    "    encoded_text, attention_mask, label = tf.py_function(encode_texts, inp=[text, label], Tout=(tf.int32, tf.int32, tf.int64))\n",
    "    \n",
    "    # `tf.data.Datasets` work best if all components have a shape set\n",
    "    encoded_text.set_shape([None])\n",
    "    attention_mask.set_shape([None])\n",
    "    label.set_shape([])\n",
    "    \n",
    "    return {\"input_ids\": encoded_text, \"attention_mask\": attention_mask}, label\n",
    "\n",
    "train_dataset = train_dataset.map(encode_map_fn)\n",
    "test_dataset = test_dataset.map(encode_map_fn)\n",
    "\n",
    "# Convert to TensorFlow Datasets\n",
    "train_dataset = train_dataset.shuffle(10000).batch(32)\n",
    "test_dataset = test_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66d78127",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T03:40:10.977996Z",
     "start_time": "2023-06-02T03:39:26.564995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca379e5090bb41c687bc8efccfc09e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 3. Load the Transformer Model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1abc89",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-02T03:40:27.804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "# 4. Training\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, validation_data=test_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150648d5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-02T03:40:58.754Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Evaluation\n",
    "# Evaluate the model on the test set\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a009eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab2f5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56343242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1876e6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf76c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48eef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
